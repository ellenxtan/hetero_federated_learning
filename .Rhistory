binary.W <- all(coord_df$Z %in% c(0, 1))
if (is.null(debiasing.weights)) {
if (binary.W) {
debiasing.weights <- (W.orig - W.hat) / (W.hat * (1 - W.hat))
} else {
# Start by learning debiasing weights if needed.
# The goal is to estimate the variance of W given X. For binary treatments,
# we get a good implicit estimator V.hat = e.hat (1 - e.hat), and
# so this step is not needed. Note that if we use the present CAPE estimator
# with a binary treatment and set V.hat = e.hat (1 - e.hat), then we recover
# exactly the AIPW estimator of the CATE.
clusters <- if (length(myfit$clusters) > 0) {
myfit$clusters
} else {
1:length(coord_df$Y)
}
variance_forest <- grf::regression_forest(coord_df$Y,
(coord_df$Z - coord_df$Z.hat)^2,
clusters = clusters,
sample.weights = myfit$sample.weights,
num.trees = num.trees.for.weights,
ci.group.size = 1)
V.hat <- predict(variance_forest, coord_df)$predictions
debiasing.weights.all <- (coord_df$Z - coord_df$Z.hat) / V.hat
debiasing.weights <- debiasing.weights.all[subset]
}
} else if (length(debiasing.weights) == length(coord_df$Y)) {
debiasing.weights <- debiasing.weights[subset]
} else if (length(debiasing.weights) != length(subset))  {
stop("If specified, debiasing.weights must have length n or |subset|.")
}
# Form AIPW scores. Note: We are implicitly using the following
# estimates for the regression surfaces E[Y|X, W=0/1]:
# Y.hat.0 <- Y.hat - W.hat * tau.hat.pointwise
# Y.hat.1 <- Y.hat + (1 - W.hat) * tau.hat.pointwise
Y.residual <- Y.orig - (Y.hat + tau.hat.pointwise * (W.orig - W.hat))
tau.hat.pointwise + debiasing.weights * Y.residual
}
## The function is modified based on the R package `grf`
## \url{https://github.com/grf-labs/grf/blob/master/r-package/grf/R/input_utilities.R}.
ValidateSubset <- function(myfit, subset, coord_df) {
if (is.null(subset)) {
subset <- 1:nrow(coord_df)
}
if (class(subset) == "logical" && length(subset) == nrow(coord_df)) {
subset <- which(subset)
}
if (!all(subset %in% 1:nrow(coord_df))) {
stop(paste(
"If specified, subset must be a vector contained in 1:n,",
"or a boolean vector of length n."
))
}
subset
}
## The function is modified based on the R package `grf`
## \url{https://github.com/grf-labs/grf/blob/master/r-package/grf/R/input_utilities.R}.
ObservationWeights <- function(myfit, coord_df) {
# Case 1: No sample.weights
if (is.null(myfit$sample.weights)) {
if (length(myfit$clusters) == 0 || !myfit$equalize.cluster.weights) {
raw.weights <- rep(1, nrow(coord_df))
} else {
# If clustering with no sample.weights provided and equalize.cluster.weights = TRUE, then
# give each observation weight 1/cluster size, so that the total weight of each cluster is the same.
clust.factor <- factor(myfit$clusters)
inverse.counts <- 1 / as.numeric(Matrix::colSums(Matrix::sparse.model.matrix(~ clust.factor + 0)))
raw.weights <- inverse.counts[as.numeric(clust.factor)]
}
}
# Case 2: sample.weights provided
if (!is.null(myfit$sample.weights)) {
if (length(myfit$clusters) == 0 || !myfit$equalize.cluster.weights) {
raw.weights <- myfit$sample.weights
} else {
stop("Specifying non-null sample.weights is not allowed when equalize.cluster.weights = TRUE")
}
}
return (raw.weights / sum(raw.weights))
}
BestLinearProj(myfit, coord_df, coord_id, site, "Z", "Y", covars, site_enc_tab=site_enc_tab)
myfit <- EnsemForest(coord_id, aug_df, "site", covars, importance="impurity")$myfit
BestLinearProj(myfit, coord_df, coord_id, site, "Z", "Y", covars)
## mean encoding (grf)
res_ef <- EnsemForest(coord_id, aug_df, "site", covars, is_encode=TRUE)
myfit <- res_ef$myfit
site_enc_tab <- res_ef$site_enc_tab
BestLinearProj(myfit, coord_df, coord_id, site, "Z", "Y", covars, site_enc_tab=site_enc_tab)
rm(list=ls())
## check functions
document()
load_all()
check()
data(SimDataLst)
K <- length(SimDataLst)
covars <- grep("^X", names(SimDataLst[[1]]), value=TRUE)
fit_lst <- list()
for (k in 1:K) {
tmpdf <- SimDataLst[[k]]
fit_lst[[k]] <- grf::causal_forest(X=as.matrix(tmpdf[, covars, with=FALSE]),
Y=tmpdf$Y, W=tmpdf$Z)
}
coord_id <- 1
coord_df <- SimDataLst[[coord_id]]
aug_df <- GenAugData(coord_id, coord_df, fit_lst, covars)
## mean encoding (grf)
res_ef <- EnsemForest(coord_id, aug_df, "site", covars, is_encode=TRUE)
myfit <- res_ef$myfit
site_enc_tab <- res_ef$site_enc_tab
PlotForestImp(myfit)
grf::variable_importance(myfit)
names(myfit$X.orig)
fit_imp <- cbind(names(myfit$X.orig), grf::variable_importance(myfit))
fit_imp
fit_imp
sum(fit_imp[,2])
class(fit_imp[,2])
sum(as.numeric(fit_imp[,2]))
names(fit_imp) <- c("names", "prop")
fit_imp
fit_imp <- as.data.frame(cbind(names(myfit$X.orig), grf::variable_importance(myfit)))
names(fit_imp) <- c("names", "prop")
fit_imp
plt_imp <- fit_imp %>% ggplot(aes(reorder(names, prop), prop)) +
geom_col() +
coord_flip() +
ylab("Relative importance") + xlab("Covariates") +
scale_x_discrete(expand = c(0, 0)) +
scale_y_continuous(expand = c(0, 0)) +
theme_classic() +
theme(axis.text=element_text(size=13,face="bold"),
axis.title=element_text(size=14,face="bold"))
plt_imp
fit_imp <- as.data.frame(cbind(names(myfit$X.orig),
as.numeric(grf::variable_importance(myfit))))
names(fit_imp) <- c("names", "prop")
plt_imp <- fit_imp %>% ggplot(aes(reorder(names, prop), prop)) +
geom_col() +
coord_flip() +
ylab("Relative importance") + xlab("Covariates") +
scale_x_discrete(expand = c(0, 0)) +
scale_y_continuous(expand = c(0, 0)) +
theme_classic() +
theme(axis.text=element_text(size=13,face="bold"),
axis.title=element_text(size=14,face="bold"))
plt_imp
fit_imp
fit_imp %>% ggplot(aes(reorder(names, prop), prop))
myfit2 <- EnsemForest(coord_id, aug_df, "site", covars, importance="impurity")$myfit
fit_imp2 <- myfit$variable.importance %>%
as.data.frame() %>%
dplyr::arrange(dplyr::desc(.)) %>%
dplyr::mutate(prop = . / sum(.)) %>%
tibble::rownames_to_column(var = "names")
fit_imp2 <- myfit2$variable.importance %>%
as.data.frame() %>%
dplyr::arrange(dplyr::desc(.)) %>%
dplyr::mutate(prop = . / sum(.)) %>%
tibble::rownames_to_column(var = "names")
View(fit_imp2)
class(fit_imp2$names)
class(fit_imp2$prop)
class(fit_imp$names)
class(fit_imp$prop)
fit_imp <- as.data.frame(cbind(names(myfit$X.orig),
as.numeric(grf::variable_importance(myfit))))
class(fit_imp$prop)
fit_imp
class(fit_imp$V2)
fit_imp <- as.data.frame(cbind(names(myfit$X.orig),
grf::variable_importance(myfit)))
names(fit_imp) <- c("names", "prop")
fit_imp$prop <- as.numeric(fit_imp$prop)
fit_imp
plt_imp <- fit_imp %>% ggplot(aes(reorder(names, prop), prop)) +
geom_col() +
coord_flip() +
ylab("Relative importance") + xlab("Covariates") +
scale_x_discrete(expand = c(0, 0)) +
scale_y_continuous(expand = c(0, 0)) +
theme_classic() +
theme(axis.text=element_text(size=13,face="bold"),
axis.title=element_text(size=14,face="bold"))
plt_imp
fit_imp2
fit_imp2 <- myfit2$variable.importance %>%
as.data.frame() %>%
dplyr::arrange(dplyr::desc(.)) %>%
dplyr::mutate(prop = . / sum(.)) %>%
tibble::rownames_to_column(var = "names") %>%
dplyr::select(-.)
fit_imp2
#'
#' ## Treat each site as a distinct factor
#' myfit <- EnsemForest(coord_id, aug_df, "site", covars, importance="impurity")$myfit
#' PlotForestImp(myfit)
#'
#' ## Mean encoding as surrogate for site index
#' myfit <- EnsemForest(coord_id, aug_df, "site", covars, is_encode=TRUE)$myfit
#' PlotForestImp(myfit)
#'
#'
PlotForestImp <- function(myfit) {
# bind variables locally to the function (define as NULL)
. <- prop <- NULL
if ("ranger" %in% class(myfit)) {
fit_imp <- myfit$variable.importance %>%
as.data.frame() %>%
dplyr::arrange(dplyr::desc(.)) %>%
dplyr::mutate(prop = . / sum(.)) %>%
tibble::rownames_to_column(var = "names") %>%
dplyr::select(-.)
}
if ("grf" %in% class(myfit)) {
fit_imp <- as.data.frame(cbind(names(myfit$X.orig),
grf::variable_importance(myfit)))
names(fit_imp) <- c("names", "prop")
fit_imp$prop <- as.numeric(fit_imp$prop)
}
plt_imp <- fit_imp %>% ggplot(aes(reorder(names, prop), prop)) +
geom_col() +
coord_flip() +
ylab("Relative importance") + xlab("Covariates") +
scale_x_discrete(expand = c(0, 0)) +
scale_y_continuous(expand = c(0, 0)) +
theme_classic() +
theme(axis.text=element_text(size=13,face="bold"),
axis.title=element_text(size=14,face="bold"))
return(plt_imp, fit_imp)
}
## mean encoding (grf)
res_ef <- EnsemForest(coord_id, aug_df, "site", covars, is_encode=TRUE)
myfit <- res_ef$myfit
BestLinearProj(myfit, coord_df, coord_id, site, "Z", "Y", covars, site_enc_tab=site_enc_tab)
BestLinearProj(myfit, coord_df, coord_id, "site", "Z", "Y", covars, site_enc_tab=site_enc_tab)
## dintinct factor (ranger)
myfit <- EnsemForest(coord_id, aug_df, "site", covars)$myfit
myfit <- EnsemForest(coord_id, aug_df, "site", covars, importance="impurity")$myfit
PlotForestImp(myfit)
#'
#' ## Treat each site as a distinct factor
#' myfit <- EnsemForest(coord_id, aug_df, "site", covars, importance="impurity")$myfit
#' PlotForestImp(myfit)
#'
#' ## Mean encoding as surrogate for site index
#' myfit <- EnsemForest(coord_id, aug_df, "site", covars, is_encode=TRUE)$myfit
#' PlotForestImp(myfit)
#'
#'
PlotForestImp <- function(myfit) {
# bind variables locally to the function (define as NULL)
. <- prop <- NULL
if ("ranger" %in% class(myfit)) {
tab_imp <- myfit$variable.importance %>%
as.data.frame() %>%
dplyr::arrange(dplyr::desc(.)) %>%
dplyr::mutate(prop = . / sum(.)) %>%
tibble::rownames_to_column(var = "names") %>%
dplyr::select(-.)
}
if ("grf" %in% class(myfit)) {
tab_imp <- as.data.frame(cbind(names(myfit$X.orig),
grf::variable_importance(myfit)))
names(tab_imp) <- c("names", "prop")
tab_imp$prop <- as.numeric(tab_imp$prop)
}
plt_imp <- tab_imp %>% ggplot(aes(reorder(names, prop), prop)) +
geom_col() +
coord_flip() +
ylab("Relative importance") + xlab("Covariates") +
scale_x_discrete(expand = c(0, 0)) +
scale_y_continuous(expand = c(0, 0)) +
theme_classic() +
theme(axis.text=element_text(size=13,face="bold"),
axis.title=element_text(size=14,face="bold"))
return(list(plt_imp=plt_imp, tab_imp=tab_imp))
}
myfit <- EnsemForest(coord_id, aug_df, "site", covars, importance="impurity")$myfit
PlotForestImp(myfit)
## mean encoding (grf)
res_ef <- EnsemForest(coord_id, aug_df, "site", covars, is_encode=TRUE)
myfit <- res_ef$myfit
PlotForestImp(myfit)
use_r("CheckHelpers")
CheckMyfit <- function(myfit) {
if (!("grf" %in% class(myfit) | "ranger" %in% class(myfit))) {
print(class(myfit))
stop("myfit should be either a `ranger` or a `grf` object")
}
if ("grf" %in% class(myfit)) {
if (!is.data.table(site_enc_tab)) {
stop("site_enc_tab should be a `data.table` when myfit is a `grf`")
}
}
return(myfit)
}
CheckMyfit(myfit)
myfit <- CheckMyfit(myfit)
?as.data.table
get(coord_df)
as.name(coord_df)
as.symbol(coord_df)
!!as.symbol(coord_df)
!!as.name(coord_df)
!as.name(coord_df)
deparse(substitute(coord_df))
?deparse
?substitute
stop(paste(deparse(substitute(df)),
"should be either a data.table/data.frame/matrix"))
df=copy(coord_df)
stop(paste(deparse(substitute(df)),
"should be either a data.table/data.frame/matrix"))
stop(paste(deparse(substitute(coord_df)),
"should be either a data.table/data.frame/matrix"))
aug_df <- CheckDF(aug_df, "aug_df")
CheckDF <- function(df, df_name) {
if (is.matrix(df) || is.data.frame(df)) {
df <- as.data.table(df)
} else {
stop(paste(df_name,
"should be either a data.table/data.frame/matrix"))
}
return(df)
}
aug_df <- CheckDF(aug_df, "aug_df")
site
site="site"
length(site)
is.vector(covars)
myfit=NULL
"ranger" %in% myfit
xpdf="norm"
if (!xpdf %in% c("unif", "norm", "binary")) {
stop("Distribution of X should be either 'unif'/'norm'/'binary' ")
}
xpdf="norm1"
if (!xpdf %in% c("unif", "norm", "binary")) {
stop("Distribution of X should be either 'unif'/'norm'/'binary' ")
}
a=1
length(a)
rm(list=ls())
## check functions
document()
load_all()
check()
data(SimDataLst)
K <- length(SimDataLst)
covars <- grep("^X", names(SimDataLst[[1]]), value=TRUE)
fit_lst <- list()
for (k in 1:K) {
tmpdf <- SimDataLst[[k]]
fit_lst[[k]] <- grf::causal_forest(X=as.matrix(tmpdf[, covars, with=FALSE]),
Y=tmpdf$Y, W=tmpdf$Z)
}
coord_id <- 1
coord_df <- SimDataLst[[coord_id]]
aug_df <- GenAugData(coord_id, coord_df, fit_lst, covars)
## dintinct factor (ranger)
myfit <- EnsemForest(coord_id, aug_df, "site", covars)$myfit
ef_hat <- EnsemForest(coord_id, coord_test, "site", covars, is_pred=TRUE,
myfit=res_ef$myfit, est_leaves=res_ef$est_leaves, honest_y=res_ef$honest_y)
#'
coord_test <- GenSimData(coord_id)
ef_hat <- EnsemForest(coord_id, coord_test, "site", covars, is_pred=TRUE,
myfit=res_ef$myfit, est_leaves=res_ef$est_leaves, honest_y=res_ef$honest_y)
res_ef <- EnsemForest(coord_id, aug_df, "site", covars)
ef_hat <- EnsemForest(coord_id, coord_test, "site", covars, is_pred=TRUE,
myfit=res_ef$myfit, est_leaves=res_ef$est_leaves, honest_y=res_ef$honest_y)
is_encode=FALSE
myfit=res_ef$myfit
est_leaves=res_ef$est_leaves
est_leaves
rm(list=ls())
## check functions
document()
## check functions
document()
load_all()
check()
## check functions
document()
load_all()
check()
data(SimDataLst)
K <- length(SimDataLst)
covars <- grep("^X", names(SimDataLst[[1]]), value=TRUE)
fit_lst <- list()
for (k in 1:K) {
tmpdf <- SimDataLst[[k]]
fit_lst[[k]] <- grf::causal_forest(X=as.matrix(tmpdf[, covars, with=FALSE]),
Y=tmpdf$Y, W=tmpdf$Z)
}
coord_id <- 1
coord_df <- SimDataLst[[coord_id]]
aug_df <- GenAugData(coord_id, coord_df, fit_lst, covars)
## TREE
res_et <- EnsemTree(coord_id, aug_df, "site", covars)
myfit <- res_et$myfit
# checking  TODO: tree & remove redundant codes in this file
CheckMyfit(myfit)
site="site"
treat="Z"
outcome="Y"
covars
X <- as.matrix(coord_df[, covars, with=FALSE])
W <- coord_df[[treat]]
Y <- coord_df[[outcome]]
coord_df$Z.hat <- regression_forest(X, W)$predictions
coord_df$Y.hat <- regression_forest(X, Y)$predictions
coord_df[[site]] <- coord_id
coord_df[[site]] <- factor(coord_df[[site]])
covar_mat <- as.matrix(coord_df[, covars, with=FALSE])
isS4(myfit)
myfit$clusters
if (!isS4(myfit)) {  # grf or ranger
clusters <- if (length(myfit$clusters) > 0) {
myfit$clusters
} else {
1:nrow(coord_df)
}
observation.weight <- ObservationWeights(myfit, coord_df)
} else {  # X-learner
clusters <- 1:nrow(coord_df)
raw.weights <- rep(1, nrow(coord_df))
observation.weight <- raw.weights / sum(raw.weights)
}
subset <- ValidateSubset(myfit, subset, coord_df)
subset=NULL
subset <- ValidateSubset(myfit, subset, coord_df)
subset.clusters <- clusters[subset]
subset.weights <- observation.weight[subset]
if (length(unique(subset.clusters)) <= 1) {
stop("The specified subset must contain units from more than one cluster.")
}
if (!is.null(debiasing.weights)) {
if (length(debiasing.weights) == nrow(coord_df)) {
debiasing.weights <- debiasing.weights[subset]
} else if (length(debiasing.weights) != length(subset)) {
stop("If specified, debiasing.weights must be a vector of length n or the subset length.")
}
}
debiasing.weights=NULL
if ("grf" %in% class(myfit)) {
coord_site_row <- which(site_enc_tab[[site]] == coord_id)
coord_df$site_enc <- site_enc_tab[coord_site_row]$site_enc
}
subset <- ValidateSubset(myfit, subset, coord_df)
W.orig <- coord_df$Z[subset]
W.hat <- coord_df$Z.hat[subset]
Y.orig <- coord_df$Y[subset]
Y.hat <- coord_df$Y.hat[subset]
class(myfit)
tau.hat.pointwise <- predict(myfit, coord_df)[subset]
names(coord_df)
View(coord_df)
binary.W <- all(coord_df$Z %in% c(0, 1))
if (is.null(debiasing.weights)) {
if (binary.W) {
debiasing.weights <- (W.orig - W.hat) / (W.hat * (1 - W.hat))
} else {
# Start by learning debiasing weights if needed.
# The goal is to estimate the variance of W given X. For binary treatments,
# we get a good implicit estimator V.hat = e.hat (1 - e.hat), and
# so this step is not needed. Note that if we use the present CAPE estimator
# with a binary treatment and set V.hat = e.hat (1 - e.hat), then we recover
# exactly the AIPW estimator of the CATE.
clusters <- if (length(myfit$clusters) > 0) {
myfit$clusters
} else {
1:length(coord_df$Y)
}
variance_forest <- grf::regression_forest(coord_df$Y,
(coord_df$Z - coord_df$Z.hat)^2,
clusters = clusters,
sample.weights = myfit$sample.weights,
num.trees = num.trees.for.weights,
ci.group.size = 1)
V.hat <- predict(variance_forest, coord_df)$predictions
debiasing.weights.all <- (coord_df$Z - coord_df$Z.hat) / V.hat
debiasing.weights <- debiasing.weights.all[subset]
}
} else if (length(debiasing.weights) == length(coord_df$Y)) {
debiasing.weights <- debiasing.weights[subset]
} else if (length(debiasing.weights) != length(subset))  {
stop("If specified, debiasing.weights must have length n or |subset|.")
}
# Form AIPW scores. Note: We are implicitly using the following
# estimates for the regression surfaces E[Y|X, W=0/1]:
# Y.hat.0 <- Y.hat - W.hat * tau.hat.pointwise
# Y.hat.1 <- Y.hat + (1 - W.hat) * tau.hat.pointwise
Y.residual <- Y.orig - (Y.hat + tau.hat.pointwise * (W.orig - W.hat))
tau.hat.pointwise + debiasing.weights * Y.residual
CheckMyfit <- function(myfit) {
if (!("grf" %in% class(myfit) |
"ranger" %in% class(myfit) |
"rpart" %in% class(myfit))) {
stop("myfit should be either a ranger/grf/rpart object")
}
}
# checking  TODO: tree & remove redundant codes in this file
CheckMyfit(myfit)
rm(list=ls())
## check functions
document()
load_all()
check()
## check functions
document()
load_all()
check()
